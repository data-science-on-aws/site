{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5756b9ac-bf41-42f3-8740-e229b916f82d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%stop_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e711d6-b13b-49e1-806e-df1b38307d37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%glue_ray\n",
    "%additional_python_modules xgboost==1.7.2,xgboost_ray\n",
    "%min_workers 25\n",
    "%number_of_workers 25\n",
    "%object_memory_worker 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8132e7ad-9c9d-4175-a3a3-1e68fc455377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226be463-358c-47c2-a14b-f403f66d9233",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "dataset = ray.data.read_parquet([\n",
    "#    's3://dsoaws/nyc-taxi-orig-cleaned-split-parquet-all-years/', # fails with \"disk full\" error\n",
    "    's3://dsoaws/nyc-taxi-orig-cleaned-parquet-all-years/', \n",
    "    ], \n",
    "#    columns=columns,\n",
    "#    filter=(pa.field(\"total_amount\") >= 0.0) # this worked briefly, then stopped working for some reason\n",
    ")\n",
    "\n",
    "dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412ea7f3-380a-407f-afc6-c4f781a4f3bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92d5567-b6eb-46f8-a74d-368dac131203",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import glob\n",
    "# from xgboost_ray import RayDMatrix, RayFileType\n",
    "# import ray\n",
    "# from ray.train.xgboost import XGBoostTrainer\n",
    "# from ray.air.config import ScalingConfig\n",
    "# from ray.air.config import RunConfig\n",
    "\n",
    "# # We can also pass a list of files\n",
    "# #path = list(sorted(glob.glob(\"/root/aws-samples-for-ray/glue/data.parquet\")))\n",
    "\n",
    "# # This argument will be passed to `pd.read_parquet()`\n",
    "# columns = [\n",
    "#     \"passenger_count\",\n",
    "#     \"trip_distance\", \n",
    "#     # \"pickup_longitude\", # these are part of some years but not others\n",
    "#     # \"pickup_latitude\",\n",
    "#     # \"dropoff_longitude\", \n",
    "#     # \"dropoff_latitude\",\n",
    "#     \"payment_type\",\n",
    "#     \"fare_amount\", \n",
    "#     \"extra\", \n",
    "#     \"mta_tax\", \n",
    "#     \"tip_amount\",\n",
    "#     \"tolls_amount\", \n",
    "#     \"total_amount\"\n",
    "# ]\n",
    "# # .drop_columns(cols=[\"vendor_id\", \"pickup_at\", \"dropoff_at\", \"rate_code_id\", \"store_and_fwd_flag\", \"payment_type\"]) \\\n",
    "\n",
    "# # filter out any rows missing the total_amount column\n",
    "# # (the other option is to \n",
    "# #import pyarrow as pa\n",
    "# # filter_expr = (\n",
    "# #     (pa.dataset.field(\"total_amount\") >= 0.0)\n",
    "# # )\n",
    "\n",
    "# # ray.init(\n",
    "# #     _system_config={\n",
    "# #         \"max_io_workers\": 4,  # More IO workers for remote storage.\n",
    "# #         \"min_spilling_size\": 100 * 1024 * 1024,  # Spill at least 100MB at a time.\n",
    "# #         \"object_spilling_config\": json.dumps(\n",
    "# #             {\n",
    "# #               \"type\": \"smart_open\",\n",
    "# #               \"params\": {\n",
    "# #                 \"uri\": \"s3://dsoaws/ray-glue-xgboost-nyc-taxi-spill\"\n",
    "# #               },\n",
    "# #               \"buffer_size\": 100 * 1024 * 1024,  # Use a 100MB buffer for writes\n",
    "# #             },\n",
    "# #         )\n",
    "# #     },\n",
    "# # )\n",
    "\n",
    "# # [SUCCEEDED] df = spark.read.option(\"recursiveFileLookup\", \"true\").parquet(\"s3://dsoaws-databricks/nyc-taxi/2019/\")\n",
    "# # [SUCCEEDED] df = spark.read.option(\"recursiveFileLookup\", \"true\").parquet(\"s3://dsoaws-databricks/nyc-taxi/2018/\")\n",
    "# # [SUCCEEDED] df = spark.read.option(\"recursiveFileLookup\", \"true\").parquet(\"s3://dsoaws-databricks/nyc-taxi/2017/\")\n",
    "# # [SUCCEEDED] df = spark.read.option(\"recursiveFileLookup\", \"true\").parquet(\"s3://dsoaws-databricks/nyc-taxi/2016/\")\n",
    "# # [SUCCEEDED] df = spark.read.option(\"recursiveFileLookup\", \"true\").parquet(\"s3://dsoaws-databricks/nyc-taxi/2015/\")\n",
    "# # [SUCCEEDED] df = spark.read.option(\"recursiveFileLookup\", \"true\").parquet(\"s3://dsoaws-databricks/nyc-taxi/2014/\")\n",
    "# # [SUCCEEDED] df = spark.read.option(\"recursiveFileLookup\", \"true\").parquet(\"s3://dsoaws-databricks/nyc-taxi/2013/\")\n",
    "# # [SUCCEEDED] df = spark.read.option(\"recursiveFileLookup\", \"true\").parquet(\"s3://dsoaws-databricks/nyc-taxi/2012/\")\n",
    "\n",
    "# # Note:  I removed [FAILED] directories from s3://dsoaws-databricks/...\n",
    "\n",
    "# # [FAILED] df = spark.read.option(\"recursiveFileLookup\", \"true\").parquet(\"s3://dsoaws-databricks/nyc-taxi/2011/\")\n",
    "# # [FAILED] df = spark.read.option(\"recursiveFileLookup\", \"true\").parquet(\"s3://dsoaws-databricks/nyc-taxi/2010/\")\n",
    "# # [FAILED] df = spark.read.option(\"recursiveFileLookup\", \"true\").parquet(\"s3://dsoaws-databricks/nyc-taxi/2009/\")\n",
    "\n",
    "# # df = spark.read.option(\"recursiveFileLookup\", \"true\").parquet(\"s3://dsoaws-databricks/nyc-taxi/\")\n",
    "\n",
    "# # dataset = ray.data.read_parquet([\n",
    "# #     's3://dsoaws/nyc-taxi/ride-metadata/year=2012/',\n",
    "# #     ], \n",
    "# #     columns=columns,\n",
    "# # #    filter=(pa.field(\"total_amount\") >= 0.0) # this worked briefly, then stopped working for some reason\n",
    "# # )\n",
    "\n",
    "# dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a744956-43c8-45e0-bd54-c1f1b54328ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def map_fn(batch: pd.DataFrame) -> pd.DataFrame:\n",
    "#     batch = batch[batch.total_amount.notnull()]\n",
    "#     return batch\n",
    "\n",
    "# dataset = dataset.map_batches(map_fn)\n",
    "\n",
    "# dataset.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2108aa9c-acaa-4e21-86a3-6c827843e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, valid_dataset = dataset.train_test_split(test_size=0.3)\n",
    "\n",
    "print(f'Train dataset size is {train_dataset.count()} and Validation dataset size is {valid_dataset.count()}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7426b3c-c18e-4c07-b010-07650551f426",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_workers = 25\n",
    "# train_dataset.repartition(num_workers)\n",
    "# valid_dataset.repartition(num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa1cecf-226c-4c8b-86d5-ba7b1898bab6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from xgboost_ray import RayDMatrix, RayFileType\n",
    "from ray.train.xgboost import XGBoostTrainer\n",
    "from ray.air.config import ScalingConfig\n",
    "from ray.air.config import RunConfig\n",
    "\n",
    "trainer = XGBoostTrainer(\n",
    "    scaling_config=ScalingConfig(\n",
    "        # Number of workers to use for data parallelism.\n",
    "        num_workers=num_workers, #int(num_workers * 8 * 0.8),\n",
    "        # Whether to use GPU acceleration.\n",
    "#        trainer_resources={\"CPU\": 0},\n",
    "        resources_per_worker={\"CPU\": 6},\n",
    "        use_gpu=False,\n",
    "        # Leave enough CPU for I/O.  80% is a good choice. \n",
    "        # (Ray will warn you if you don't leave enough CPU for I/O.)\n",
    "        _max_cpu_fraction_per_node = 0.8\n",
    "    ),\n",
    "    run_config=RunConfig(local_dir=\"/tmp/ray_results\"),\n",
    "    label_column='total_amount',\n",
    "    num_boost_round=5,\n",
    "    params={\n",
    "        # XGBoost specific params\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": [\"rmse\", \"mae\"]\n",
    "    },\n",
    "    datasets={\"train\": train_dataset, \"valid\": valid_dataset},\n",
    ")\n",
    "\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daec40de-c934-485d-a240-3f5b3c9c84fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(result.metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da462aa1-5476-4e66-b336-ede73750318f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('/tmp/ray_results/XGBoostTrainer_2023-01-09_21-11-06/XGBoostTrainer_21b7c_00000_0_2023-01-09_21-11-06/error.txt', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca48dbb-e005-4127-a5ed-9dced3c4afbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Glue Python [PySpark and Ray] (SparkAnalytics 1.0)",
   "language": "python",
   "name": "conda-env-sm_glue_is-glue_pyspark__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-sparkanalytics-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
