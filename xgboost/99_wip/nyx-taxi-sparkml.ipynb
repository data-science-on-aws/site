{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "source": [
    "\n",
    "# Glue Studio Notebook\n",
    "You are now running a **Glue Studio** notebook; before you can start using your notebook you *must* start an interactive session.\n",
    "\n",
    "## Available Magics\n",
    "|          Magic              |   Type       |                                                                        Description                                                                        |\n",
    "|-----------------------------|--------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| %%configure                 |  Dictionary  |  A json-formatted dictionary consisting of all configuration parameters for a session. Each parameter can be specified here or through individual magics. |\n",
    "| %profile                    |  String      |  Specify a profile in your aws configuration to use as the credentials provider.                                                                          |\n",
    "| %iam_role                   |  String      |  Specify an IAM role to execute your session with.                                                                                                        |\n",
    "| %region                     |  String      |  Specify the AWS region in which to initialize a session.                                                                                                 |\n",
    "| %session_id                 |  String      |  Returns the session ID for the running session.                                                                                                          |\n",
    "| %connections                |  List        |  Specify a comma separated list of connections to use in the session.                                                                                     |\n",
    "| %additional_python_modules  |  List        |  Comma separated list of pip packages, s3 paths or private pip arguments.                                                                                 |\n",
    "| %extra_py_files             |  List        |  Comma separated list of additional Python files from S3.                                                                                                 |\n",
    "| %extra_jars                 |  List        |  Comma separated list of additional Jars to include in the cluster.                                                                                       |\n",
    "| %number_of_workers          |  Integer     |  The number of workers of a defined worker_type that are allocated when a job runs. worker_type must be set too.                                          |\n",
    "| %glue_version               |  String      |  The version of Glue to be used by this session. Currently, the only valid options are 2.0 and 3.0 (eg: %glue_version 2.0).                               |\n",
    "| %security_config            |  String      |  Define a security configuration to be used with this session.                                                                                            |\n",
    "| %sql                        |  String      |  Run SQL code. All lines after the initial %%sql magic will be passed as part of the SQL code.                                                            |\n",
    "| %streaming                  |  String      |  Changes the session type to Glue Streaming.                                                                                                              |\n",
    "| %etl                        |  String      |  Changes the session type to Glue ETL.                                                                                                                    |\n",
    "| %status                     |              |  Returns the status of the current Glue session including its duration, configuration and executing user / role.                                          |\n",
    "| %stop_session               |              |  Stops the current session.                                                                                                                               |\n",
    "| %list_sessions              |              |  Lists all currently running sessions by name and ID.                                                                                                     |\n",
    "| %worker_type                |  String      |  Standard, G.1X, *or* G.2X. number_of_workers must be set too. Default is G.1X.                                                                           |\n",
    "| %spark_conf                 |  String      |  Specify custom spark configurations for your session. E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer.                      |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%stop_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%glue_version 3.0\n",
    "%number_of_workers 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# from awsglue.transforms import *\n",
    "# from awsglue.utils import getResolvedOptions\n",
    "# from pyspark.context import SparkContext\n",
    "# from awsglue.context import GlueContext\n",
    "# from awsglue.job import Job\n",
    "\n",
    "# sc = SparkContext.getOrCreate()\n",
    "# glueContext = GlueContext(sc)\n",
    "# spark = glueContext.spark_session\n",
    "# job = Job(glueContext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = spark.read.parquet(\"s3://dsoaws/nyc-taxi-orig-cleaned-parquet-all-years/part-00000-tid-6910082926969615124-943bbb2c-0113-433e-a049-0be51a94a63e-841-1-c000.snappy.parquet\")\n",
    "\n",
    "# The following command caches the DataFrame in memory. This improves performance since subsequent calls to the DataFrame can read from memory instead of re-reading the data from disk.\n",
    "#df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"The dataset has %d rows.\" % data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "\n",
    "# Remove the target column from the input feature set.\n",
    "featuresCols = data.columns\n",
    "featuresCols.remove('total_amount')\n",
    "featuresCols.remove('dropoff_at')\n",
    "featuresCols.remove('pickup_at')\n",
    "featuresCols.remove('store_and_fwd_flag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vectorAssembler combines all feature columns into a single feature vector column, \"rawFeatures\".\n",
    "vectorAssembler = VectorAssembler(inputCols=featuresCols, \n",
    "                                  outputCol=\"rawFeatures\", \n",
    "                                  handleInvalid=\"skip\")\n",
    "\n",
    "# vectorIndexer identifies categorical features and indexes them, and creates a new column \"features\". \n",
    "vectorIndexer = VectorIndexer(inputCol=\"rawFeatures\", \n",
    "                              outputCol=\"features\", \n",
    "                              maxCategories=100, \n",
    "                              handleInvalid=\"skip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "# featureIndexer =\\\n",
    "#     VectorIndexer(inputCol=\"rawFeatures\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestRegressor(labelCol=\"total_amount\", featuresCol=\"features\")\n",
    "\n",
    "# Chain indexer and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[vectorAssembler, vectorIndexer, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainingData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "testData.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexer.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO:  the testData may not have been transformed since \n",
    "#        it was not part of the pipeline above which performs the indexing\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"total_amount\", \"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"total_amount\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
    "\n",
    "rfModel = model.stages[1]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Glue Python [PySpark and Ray] (SparkAnalytics 1.0)",
   "language": "python",
   "name": "conda-env-sm_glue_is-glue_pyspark__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-sparkanalytics-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "Python_Glue_Session",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
